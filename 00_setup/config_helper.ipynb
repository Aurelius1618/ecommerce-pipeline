{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc177f7-0ed1-4def-826a-3baa0692398f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13459c1c-3639-401c-af62-f691bf1d46ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transaction schema\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"unit_price\", DoubleType(), False),\n",
    "    StructField(\"total_amount\", DoubleType(), False),\n",
    "    StructField(\"transaction_date\", DateType(), False),\n",
    "    StructField(\"channel\", StringType(), False),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"shipping_address\", StringType(), True),\n",
    "    StructField(\"campaign_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Product schema\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"subcategory\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"cost_price\", DoubleType(), False),\n",
    "    StructField(\"retail_price\", DoubleType(), False),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Customer schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True),\n",
    "    StructField(\"registration_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Campaign schema\n",
    "campaign_schema = StructType([\n",
    "    StructField(\"campaign_id\", StringType(), False),\n",
    "    StructField(\"campaign_name\", StringType(), False),\n",
    "    StructField(\"campaign_type\", StringType(), False),\n",
    "    StructField(\"start_date\", DateType(), False),\n",
    "    StructField(\"end_date\", DateType(), False),\n",
    "    StructField(\"budget\", DoubleType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"target_audience\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90b7038-9c50-43b3-8d49-6d280da3e73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_csv_with_schema(file_path, schema, header=True):\n",
    "    \"\"\"\n",
    "    Load CSV file with predefined schema and error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to CSV file\n",
    "        schema (StructType): Predefined schema\n",
    "        header (bool): Whether CSV has header row\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Loaded and validated DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading CSV from: {file_path}\")\n",
    "        \n",
    "        df = (spark.read\n",
    "              .option(\"header\", header)\n",
    "              .option(\"inferSchema\", False)\n",
    "              .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "              .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "              .schema(schema)\n",
    "              .csv(file_path))\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {df.count()} rows from {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV {file_path}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57bc5847-f83a-44e7-a8d6-68ad1347fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_audit_columns(df):\n",
    "    \"\"\"\n",
    "    Add audit columns to DataFrame for tracking\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with audit columns added\n",
    "    \"\"\"\n",
    "    return (df\n",
    "            .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "            .withColumn(\"ingestion_date\", F.current_date())\n",
    "            .withColumn(\"source_file\", F.input_file_name())\n",
    "            .withColumn(\"pipeline_run_id\", F.lit(spark.sparkContext.applicationId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0aa9a56-0a61-4228-818d-030d961c02b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(df, table_name, mode=\"overwrite\", partition_cols=None, optimize_write=True):\n",
    "    \"\"\"\n",
    "    Write DataFrame to Delta Lake table with optimizations\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to write\n",
    "        table_name (str): Name of Delta table\n",
    "        mode (str): Write mode (overwrite, append, merge)\n",
    "        partition_cols (list): Columns to partition by\n",
    "        optimize_write (bool): Enable optimize write\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Writing to Delta table: {table_name}\")\n",
    "        \n",
    "        writer = df.write.format(\"delta\").mode(mode)\n",
    "        \n",
    "        if partition_cols:\n",
    "            writer = writer.partitionBy(partition_cols)\n",
    "        \n",
    "        if optimize_write:\n",
    "            writer = writer.option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "        \n",
    "        writer.saveAsTable(table_name)\n",
    "        \n",
    "        logger.info(f\"Successfully wrote {df.count()} rows to {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to Delta table {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9ab9a9-3ef7-4072-8291-a5b8f36d71ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_database_if_not_exists(database_name):\n",
    "    \"\"\"\n",
    "    Create database if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        database_name (str): Name of database to create\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        logger.info(f\"Database {database_name} created or already exists\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating database {database_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c75de4-f1a7-4bab-881c-13ecac631671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_data_quality(df, table_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality checks\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to check\n",
    "        table_name (str): Name of table for logging\n",
    "    \n",
    "    Returns:\n",
    "        dict: Data quality metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Performing data quality checks for {table_name}\")\n",
    "        \n",
    "        total_rows = df.count()\n",
    "        total_cols = len(df.columns)\n",
    "        \n",
    "        # Check for null values\n",
    "        null_counts = {}\n",
    "        for col in df.columns:\n",
    "            null_count = df.filter(F.col(col).isNull()).count()\n",
    "            null_counts[col] = null_count\n",
    "        \n",
    "        # Check for duplicates (if primary key exists)\n",
    "        duplicate_count = 0\n",
    "        if 'transaction_id' in df.columns:\n",
    "            duplicate_count = df.count() - df.dropDuplicates(['transaction_id']).count()\n",
    "        elif 'product_id' in df.columns:\n",
    "            duplicate_count = df.count() - df.dropDuplicates(['product_id']).count()\n",
    "        elif 'customer_id' in df.columns:\n",
    "            duplicate_count = df.count() - df.dropDuplicates(['customer_id']).count()\n",
    "        \n",
    "        quality_metrics = {\n",
    "            'table_name': table_name,\n",
    "            'total_rows': total_rows,\n",
    "            'total_columns': total_cols,\n",
    "            'null_counts': null_counts,\n",
    "            'duplicate_count': duplicate_count,\n",
    "            'check_timestamp': F.current_timestamp()\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Data quality check completed for {table_name}\")\n",
    "        return quality_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data quality check for {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42ff6b2-20e5-4ec0-8e67-fbb46ae1d180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Database names\n",
    "BRONZE_DB = \"bronze\"\n",
    "SILVER_DB = \"silver\"\n",
    "GOLD_DB = \"gold\"\n",
    "\n",
    "# Mount point\n",
    "MOUNT_POINT = \"/mnt/ecommerce\"\n",
    "\n",
    "# File paths\n",
    "TRANSACTION_PATH = f\"{MOUNT_POINT}/transactions/*.csv\"\n",
    "PRODUCT_PATH = f\"{MOUNT_POINT}/products/*.csv\"\n",
    "CUSTOMER_PATH = f\"{MOUNT_POINT}/customers/*.csv\"\n",
    "CAMPAIGN_PATH = f\"{MOUNT_POINT}/campaigns/*.csv\"\n",
    "\n",
    "# Create databases\n",
    "create_database_if_not_exists(BRONZE_DB)\n",
    "create_database_if_not_exists(SILVER_DB)\n",
    "create_database_if_not_exists(GOLD_DB)\n",
    "\n",
    "print(\"✅ Configuration and helper functions loaded successfully!\")\n",
    "print(f\"✅ Databases created: {BRONZE_DB}, {SILVER_DB}, {GOLD_DB}\")\n",
    "print(f\"✅ Mount point configured: {MOUNT_POINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b484607f-112e-4079-9674-3955d4e9d021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Execution Results (Manual Documentation)\n",
    "### Bronze database created successfully\n",
    "### Silver database created successfully  \n",
    "### Gold database created successfully\n",
    "### Mount point configured: /mnt/ecommerce\n",
    "### All helper functions loaded and tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9c4c24-1679-44e2-99cb-a05fb080b35f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify databases exist\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Test a helper function\n",
    "print(\"Transaction schema fields:\")\n",
    "for field in transaction_schema.fields:\n",
    "    print(f\"- {field.name}: {field.dataType}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "config_helper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
